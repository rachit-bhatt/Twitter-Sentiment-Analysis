{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <font color = '#00FFFF'>\n",
    "        <h1>Tweets Classification</h1>\n",
    "    </font>\n",
    "    <font color = '#FFA500'>\n",
    "        <h3>Natural Language Processing</h3>\n",
    "    </font>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#region Downloading Dataset\n",
    "\n",
    "# Operating Systemm.\n",
    "from os import path, environ, listdir, remove\n",
    "\n",
    "# JSON.\n",
    "from json import load\n",
    "\n",
    "# ZIP-File.\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Input-Output.\n",
    "from io import BytesIO\n",
    "\n",
    "# Requests.\n",
    "from requests import get, Session\n",
    "\n",
    "# Kaggle API.\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "#endregion\n",
    "\n",
    "#region Data Manipulation\n",
    "\n",
    "# Pandas.\n",
    "from pandas import read_csv, DataFrame, Series\n",
    "\n",
    "# Numpy.\n",
    "from numpy import NAN\n",
    "\n",
    "# Randomization.\n",
    "from random import random\n",
    "\n",
    "# String.\n",
    "from string import ascii_letters\n",
    "\n",
    "#endregion\n",
    "\n",
    "# Regular Expression.\n",
    "from re import findall\n",
    "\n",
    "#region Visualization\n",
    "\n",
    "# Plotly-Express.\n",
    "from plotly.express import scatter\n",
    "\n",
    "# Seaborn.\n",
    "from seaborn import heatmap\n",
    "\n",
    "# Matplot-Library.\n",
    "from matplotlib.pyplot import show\n",
    "\n",
    "# Word-Cloud.\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#endregion\n",
    "\n",
    "#region Natural Language Processing\n",
    "\n",
    "#region Natural Language Toolkit\n",
    "\n",
    "# Step-Words.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenization.\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "\n",
    "# Stemming.\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "#endregion\n",
    "\n",
    "#endregion\n",
    "\n",
    "#region Machine Learning Model\n",
    "\n",
    "#region Torch\n",
    "\n",
    "#endregion\n",
    "\n",
    "#endregion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from the Kaggle platform using URL -> https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis/data.\n",
    "\n",
    "def fetch_dataset() -> DataFrame | None:\n",
    "\n",
    "    #region Configurations\n",
    "\n",
    "    # File-Name for storing the credentials.\n",
    "    api_file_path = 'Kaggle.JSON'\n",
    "    \n",
    "    # URL for dataset on Kaggle.\n",
    "    dataset_url = 'jp797498e/twitter-entity-sentiment-analysis'\n",
    "    \n",
    "    # Dataset File Name.\n",
    "    train_dataset_file_path = 'twitter_training.csv'\n",
    "    \n",
    "    #endregion\n",
    "\n",
    "    # Proceed when the file exists.\n",
    "    if path.exists(path = api_file_path):\n",
    "        \n",
    "        #region Loading the credentials from the file.\n",
    "        \n",
    "        # Loading the file for credentials.\n",
    "        with open(file = api_file_path, mode = 'r') as json_file:\n",
    "\n",
    "            # Store the file content in JSON format.\n",
    "            kaggle_api_credentials = load(json_file)\n",
    "        \n",
    "        #endregion\n",
    "        \n",
    "        #region Storing Credentials in OS Environment\n",
    "        \n",
    "        # Storing User-Name.\n",
    "        environ['KAGGLE_USERNAME'] = kaggle_api_credentials['username']\n",
    "        \n",
    "        # Storing API-Key.\n",
    "        environ['KAGGLE_KEY'] = kaggle_api_credentials['key']\n",
    "        \n",
    "        #endregion\n",
    "    \n",
    "        #region Fetching Data from Kaggle.\n",
    "\n",
    "        # Creating an instance for interacting with Kaggle.\n",
    "        kaggle_api = KaggleApi()\n",
    "        \n",
    "        # Authenticate using credentials downloaded.\n",
    "        kaggle_api.authenticate()\n",
    "        \n",
    "        # Downloading the Dataset Files.\n",
    "        kaggle_api.dataset_download_files(dataset = dataset_url, unzip = True)\n",
    "        \n",
    "        #endregion\n",
    "\n",
    "        # Reading the dataset once downloaded in the local system.\n",
    "        dataset = read_csv(filepath_or_buffer = train_dataset_file_path)\n",
    "        \n",
    "        #region Data Cleaning\n",
    "        \n",
    "        #region Dropping Kaggle API Credentials from Operating Systen's Environment\n",
    "        \n",
    "        # Dropping the User-Name.\n",
    "        del environ['KAGGLE_USERNAME']\n",
    "        \n",
    "        # Dropping the API-Key.\n",
    "        del environ['KAGGLE_KEY']\n",
    "        \n",
    "        #endregion\n",
    "        \n",
    "        #region Removing All CSV Files\n",
    "\n",
    "        # Iterating over each file in the current working directory.\n",
    "        for file in listdir():\n",
    "\n",
    "            # Proceed when found a CSV file.            \n",
    "            if file.endswith('.csv'):\n",
    "\n",
    "                # Removing CSV file found in CWD.\n",
    "                remove(file)\n",
    "        \n",
    "        #endregion\n",
    "        \n",
    "        #endregion\n",
    "        \n",
    "        # Return the stored dataset after the file has been deleted.\n",
    "        return dataset\n",
    "    \n",
    "    else:\n",
    "        print('Failed to find the file for the API credentials.\\nPlease validate the file path and OS restrictions.')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the required dataset from Kaggle.\n",
    "train_dataset = fetch_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Positive</th>\n",
       "      <th>im getting on borderlands and i will murder you all ,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2401  Borderlands  Positive  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "...     ...          ...       ...   \n",
       "74676  9200       Nvidia  Positive   \n",
       "74677  9200       Nvidia  Positive   \n",
       "74678  9200       Nvidia  Positive   \n",
       "74679  9200       Nvidia  Positive   \n",
       "74680  9200       Nvidia  Positive   \n",
       "\n",
       "      im getting on borderlands and i will murder you all ,  \n",
       "0      I am coming to the borders and I will kill you...     \n",
       "1      im getting on borderlands and i will kill you ...     \n",
       "2      im coming on borderlands and i will murder you...     \n",
       "3      im getting on borderlands 2 and i will murder ...     \n",
       "4      im getting into borderlands and i can murder y...     \n",
       "...                                                  ...     \n",
       "74676  Just realized that the Windows partition of my...     \n",
       "74677  Just realized that my Mac window partition is ...     \n",
       "74678  Just realized the windows partition of my Mac ...     \n",
       "74679  Just realized between the windows partition of...     \n",
       "74680  Just like the windows partition of my Mac is l...     \n",
       "\n",
       "[74681 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First look on the dataset.\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74681 entries, 0 to 74680\n",
      "Data columns (total 4 columns):\n",
      " #   Column                                                 Non-Null Count  Dtype \n",
      "---  ------                                                 --------------  ----- \n",
      " 0   2401                                                   74681 non-null  int64 \n",
      " 1   Borderlands                                            74681 non-null  object\n",
      " 2   Positive                                               74681 non-null  object\n",
      " 3   im getting on borderlands and i will murder you all ,  73995 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Understanding the datatypes and null-values.\n",
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Positive</th>\n",
       "      <th>im getting on borderlands and i will murder you all ,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>74681.000000</td>\n",
       "      <td>74681</td>\n",
       "      <td>74681</td>\n",
       "      <td>73995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>69490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TomClancysRainbowSix</td>\n",
       "      <td>Negative</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2400</td>\n",
       "      <td>22542</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6432.640149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3740.423819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3195.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6422.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9601.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13200.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                2401           Borderlands  Positive  \\\n",
       "count   74681.000000                 74681     74681   \n",
       "unique           NaN                    32         4   \n",
       "top              NaN  TomClancysRainbowSix  Negative   \n",
       "freq             NaN                  2400     22542   \n",
       "mean     6432.640149                   NaN       NaN   \n",
       "std      3740.423819                   NaN       NaN   \n",
       "min         1.000000                   NaN       NaN   \n",
       "25%      3195.000000                   NaN       NaN   \n",
       "50%      6422.000000                   NaN       NaN   \n",
       "75%      9601.000000                   NaN       NaN   \n",
       "max     13200.000000                   NaN       NaN   \n",
       "\n",
       "       im getting on borderlands and i will murder you all ,  \n",
       "count                                               73995     \n",
       "unique                                              69490     \n",
       "top                                                           \n",
       "freq                                                  172     \n",
       "mean                                                  NaN     \n",
       "std                                                   NaN     \n",
       "min                                                   NaN     \n",
       "25%                                                   NaN     \n",
       "50%                                                   NaN     \n",
       "75%                                                   NaN     \n",
       "max                                                   NaN     "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyzing statistics in data.\n",
    "train_dataset.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2401                                                       0\n",
       "Borderlands                                                0\n",
       "Positive                                                   0\n",
       "im getting on borderlands and i will murder you all ,    686\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing number of missing values in the dataset.\n",
    "train_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2401                                                     0.000000\n",
       "Borderlands                                              0.000000\n",
       "Positive                                                 0.000000\n",
       "im getting on borderlands and i will murder you all ,    0.918574\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying the proportion of the data missing.\n",
    "train_dataset.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Data Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2700, 3.615377405230246)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying number of duplicate records and its average.\n",
    "train_dataset.duplicated().sum(), train_dataset.duplicated().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Duplicated and Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicate records.\n",
    "train_dataset.drop_duplicates(inplace = True)\n",
    "\n",
    "# Dropping the missing values due to its proportion.\n",
    "train_dataset.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validating the operation on duplicate data.\n",
    "train_dataset.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2401                                                     0\n",
       "Borderlands                                              0\n",
       "Positive                                                 0\n",
       "im getting on borderlands and i will murder you all ,    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validating the operation on missing data.\n",
    "train_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing Feature Naming Convensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining proper name for the features.\n",
    "new_features_name = [\n",
    "    'Index',\n",
    "    'Topic',\n",
    "    'Sentiments',\n",
    "    'Text'\n",
    "]\n",
    "\n",
    "# Changing the names of the features for preventing confusion for future analysis and predictions.\n",
    "train_dataset.rename(columns = {\n",
    "                        old_feature_name : new_feature_name for old_feature_name, new_feature_name in zip(list(train_dataset.columns), new_features_name)\n",
    "                     },\n",
    "                     inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Index', 'Topic', 'Sentiments', 'Text'], dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validating the renaming operation.\n",
    "train_dataset.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
